{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21098a4d",
   "metadata": {},
   "source": [
    "# Part 1 - Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fadb30",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "084973e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import html5lib\n",
    "import requests\n",
    "import regex as re\n",
    "import bs4\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6b5aca",
   "metadata": {},
   "source": [
    "### Reading from CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de8f0fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('url_technology(1).csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96bc284",
   "metadata": {},
   "source": [
    "### CSV file having URLs to scrap looks like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "725c3e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://medium.com/javascript-scene/top-javasc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://medium.com/job-advice-for-software-eng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://itnext.io/load-testing-using-apache-jm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://medium.com/s/story/black-mirror-bander...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://medium.com/fast-company/the-worst-desi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58093</th>\n",
       "      <td>https://medium.com/@christopherthomson/i-would...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58094</th>\n",
       "      <td>https://medium.com/@sportulaproducts1/sportula...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58095</th>\n",
       "      <td>https://medium.com/@thesupergamercorpus/as-som...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58096</th>\n",
       "      <td>https://medium.com/swlh/do-writing-aids-such-a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58097</th>\n",
       "      <td>https://medium.com/tenderlymag/are-cats-liquid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>58098 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     url\n",
       "0      https://medium.com/javascript-scene/top-javasc...\n",
       "1      https://medium.com/job-advice-for-software-eng...\n",
       "2      https://itnext.io/load-testing-using-apache-jm...\n",
       "3      https://medium.com/s/story/black-mirror-bander...\n",
       "4      https://medium.com/fast-company/the-worst-desi...\n",
       "...                                                  ...\n",
       "58093  https://medium.com/@christopherthomson/i-would...\n",
       "58094  https://medium.com/@sportulaproducts1/sportula...\n",
       "58095  https://medium.com/@thesupergamercorpus/as-som...\n",
       "58096  https://medium.com/swlh/do-writing-aids-such-a...\n",
       "58097  https://medium.com/tenderlymag/are-cats-liquid...\n",
       "\n",
       "[58098 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6913f6d8",
   "metadata": {},
   "source": [
    "### Defining dataframe and implementing scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2bc8c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "4\n",
      "5\n",
      "8\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "71\n",
      "72\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "104\n",
      "106\n",
      "107\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "115\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "129\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "141\n",
      "142\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "151\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "160\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "171\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "193\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "200\n",
      "202\n",
      "203\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "231\n",
      "233\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "246\n",
      "248\n",
      "249\n",
      "252\n",
      "253\n",
      "254\n",
      "256\n",
      "258\n",
      "260\n",
      "261\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "281\n",
      "282\n",
      "284\n",
      "285\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "308\n",
      "309\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "355\n",
      "357\n",
      "358\n",
      "359\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "387\n",
      "388\n",
      "390\n",
      "391\n",
      "392\n",
      "394\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "418\n",
      "419\n",
      "421\n",
      "422\n",
      "423\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "432\n",
      "433\n",
      "435\n",
      "437\n",
      "438\n",
      "441\n",
      "442\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "460\n",
      "461\n",
      "463\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "477\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "491\n",
      "492\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "535\n",
      "536\n",
      "540\n",
      "541\n",
      "543\n",
      "544\n",
      "545\n",
      "547\n",
      "548\n",
      "Scraped Successfully\n"
     ]
    }
   ],
   "source": [
    "#creating dataframe and defining column names \n",
    "df = pd.DataFrame(columns = ['Index', 'URL', 'Title', 'Sub Title', 'Article', 'Author', 'Author URL','Image URL', 'Reading Time', 'Clap Count'])\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36 Edg/113.0.1774.42'}\n",
    "\n",
    "urls_scraped = []\n",
    "\n",
    "#Scrapping for 550 random URLs from CSV file\n",
    "for i in range(550):\n",
    "    \n",
    "        cur_link_data = []\n",
    "        url_index = random.randint(2, 58100)\n",
    "        url_to_scrap = data.iloc[url_index ]['url']\n",
    "#Implementing exception here to continue scrapping incase we don't get access to some URLs         \n",
    "        \n",
    "        try:\n",
    "        \n",
    "            r = requests.get(url_to_scrap, headers=headers, timeout = 20)\n",
    "            r_text = r.text\n",
    "            soup = BeautifulSoup(r_text, \"html5lib\")\n",
    "#Getting title on article's webpage. Exception added\n",
    "            try:\n",
    "                title = soup.find('h1').contents[0].text\n",
    "            except AttributeError:\n",
    "                title_raw = (soup.find('title')).text\n",
    "                title = title_raw[:title_raw.find(\"|\")]\n",
    "\n",
    "            if not title:\n",
    "                continue\n",
    "#Getting Subtitle\n",
    "            try:\n",
    "                if \"Written by\" in soup.find('h2').contents[0].text:\n",
    "                    sub_title = \"None\"\n",
    "                else:\n",
    "                    sub_title = soup.find('h2').contents[0].text\n",
    "            except AttributeError:\n",
    "                sub_title = \"None\"\n",
    "#Getting Author name\n",
    "            try:\n",
    "                author_name_raw = (soup.find(\"h2\", {\"class\": \"pw-author-name\"})).text\n",
    "                author_name = author_name_raw[11:]\n",
    "            except AttributeError:\n",
    "                author_name = \"None\"\n",
    "#Getting Author URL\n",
    "            try:\n",
    "                author_url_raw = (soup.find(\"h2\", {\"class\": \"pw-author-name\"})).parent['href']\n",
    "                author_url = \"https://medium.com\" + author_url_raw[:author_url_raw.find(\"?\")]\n",
    "            except AttributeError:\n",
    "                author_url = \"None\"\n",
    "#Getting reading time\n",
    "            r_content = r.content.decode(\"utf-8\")\n",
    "            reading_time = r_content[r_content.find(\" min read\")-2:r_content.find(\" min read\")]\n",
    "            if reading_time[0] == '\"':\n",
    "                reading_time = reading_time[1:]\n",
    "#Getting clap count\n",
    "            try:\n",
    "                clap_count = r_content.split(\"clapCount\\\":\")[1]\n",
    "            except IndexError:\n",
    "                clap_count = r_content.split(\"clapCount\\\":\")[0]\n",
    "            clap_count = clap_count[0:clap_count.find(\",\")]\n",
    "#Getting images link\n",
    "            reg = re.compile('(https:\\/\\/miro\\.medium\\.com\\/v2\\/resize:fit:)[\\d]+(\\/)(format:webp\\/|)(\\w|\\d|-|\\*)+(\\.jpeg|\\.png|\\.gif)')\n",
    "            img_tags = soup.find_all('picture')\n",
    "            img_srcs = str(len(img_tags))\n",
    "            for img in img_tags:\n",
    "                mos = reg.search(str(img))\n",
    "                try:\n",
    "                    img_srcs += \", \" + str(mos[0])\n",
    "                except TypeError:\n",
    "                    continue\n",
    "#Getting article's content\n",
    "            text = \"\"\n",
    "            divTag = soup.find_all(\"div\", {\"class\": \"ch bg dx dy dz ea\"})\n",
    "            for tag in divTag:\n",
    "                tdTags = tag.find_all(\"p\")\n",
    "                for tag in tdTags:\n",
    "                    text += tag.text\n",
    "\n",
    "            if not text:\n",
    "                continue\n",
    "#Appending and saving the scraped data in dataframe\n",
    "            df.loc[i] = [str(url_index), url_to_scrap, title, sub_title, text, author_name, author_url, img_srcs, reading_time, clap_count]\n",
    "            urls_scraped.append(url_to_scrap)\n",
    "            print(i)\n",
    "#Sleep time added. Our scrapper should wait 1 and 5 seconds between requests            \n",
    "            time.sleep(1)\n",
    "        \n",
    "        except:\n",
    "            df.to_csv(\"data_scraped.csv\", encoding='utf-8')\n",
    "            time.sleep(5)\n",
    "#Creating a csv file of our scrapped data, from our dataframe.\n",
    "df.to_csv(\"data_scraped.csv\", encoding='utf-8')\n",
    "print(\"Scraped Successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3737ce",
   "metadata": {},
   "source": [
    "### Overview of our scrapped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "407e7b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_data= pd.read_csv('data_scraped.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a4a3829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Index</th>\n",
       "      <th>URL</th>\n",
       "      <th>Title</th>\n",
       "      <th>Sub Title</th>\n",
       "      <th>Article</th>\n",
       "      <th>Author</th>\n",
       "      <th>Author URL</th>\n",
       "      <th>Image URL</th>\n",
       "      <th>Reading Time</th>\n",
       "      <th>Clap Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>12925</td>\n",
       "      <td>https://medium.com/@dalwrobinson/where-the-str...</td>\n",
       "      <td>Where the streets have new names</td>\n",
       "      <td>None</td>\n",
       "      <td>The Hall PressFollowHall Associated Publicatio...</td>\n",
       "      <td>The Hall Press</td>\n",
       "      <td>https://medium.com/@dalwrobinson</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>42280</td>\n",
       "      <td>https://medium.com/microsoft-cybersecurity/wei...</td>\n",
       "      <td>Weighing Past and Future Success</td>\n",
       "      <td>Nothing worthwhile is easy. What are you willi...</td>\n",
       "      <td>Lucas DowdFollowMicrosoft Cybersecurity--Liste...</td>\n",
       "      <td>Lucas Dowd</td>\n",
       "      <td>https://medium.com/@lucas.dowd</td>\n",
       "      <td>1, https://miro.medium.com/v2/resize:fit:640/f...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1753</td>\n",
       "      <td>https://medium.com/@rohela99/microservice-canv...</td>\n",
       "      <td>Microservice Canvas</td>\n",
       "      <td>None</td>\n",
       "      <td>RohelaFollow--ListenShareEvery business analys...</td>\n",
       "      <td>Rohela</td>\n",
       "      <td>https://medium.com/</td>\n",
       "      <td>3, https://miro.medium.com/v2/resize:fit:640/f...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>6769</td>\n",
       "      <td>https://medium.com/@chevallier/the-road-to-cto...</td>\n",
       "      <td>The Road to CTO (#3)</td>\n",
       "      <td>This ghost wonâ€™t escape me much longer ðŸ‘»</td>\n",
       "      <td>JÃ©rÃ©my ChevallierFollow--ListenShareItâ€™s encou...</td>\n",
       "      <td>JÃ©rÃ©my Chevallier</td>\n",
       "      <td>https://medium.com/@chevallier</td>\n",
       "      <td>2, https://miro.medium.com/v2/resize:fit:640/0...</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>28240</td>\n",
       "      <td>https://medium.com/@roblevintennis/this-was-fu...</td>\n",
       "      <td>This was fun! I challenged myself to do them i...</td>\n",
       "      <td>None</td>\n",
       "      <td>Ady NgomRob LevinFollow--2ListenShareThis was ...</td>\n",
       "      <td>Rob Levin</td>\n",
       "      <td>https://medium.com/</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>543</td>\n",
       "      <td>31523</td>\n",
       "      <td>https://medium.com/active-theory/netflix-our-p...</td>\n",
       "      <td>Netflix: Our Planet</td>\n",
       "      <td>None</td>\n",
       "      <td>Member-only storyActive TheoryFollowActive The...</td>\n",
       "      <td>Active Theory</td>\n",
       "      <td>https://medium.com/@activetheory</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>544</td>\n",
       "      <td>48943</td>\n",
       "      <td>https://medium.com/@livewellmumbai/android-10-...</td>\n",
       "      <td>Android 10 update arrives on the Nokia 8.1</td>\n",
       "      <td>None</td>\n",
       "      <td>Sanket Ramesh PrasadeFollow--ListenShareAndroi...</td>\n",
       "      <td>Sanket Ramesh Prasade</td>\n",
       "      <td>https://medium.com/@livewellmumbai</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>545</td>\n",
       "      <td>12711</td>\n",
       "      <td>https://medium.com/@avonleafisher/go-live-in-a...</td>\n",
       "      <td>â€œGo Live in a Cave if Youâ€™re Really Anti-Capit...</td>\n",
       "      <td>None</td>\n",
       "      <td>Avonlea FisherFollow--ListenShareWhether or no...</td>\n",
       "      <td>Avonlea Fisher</td>\n",
       "      <td>https://medium.com/</td>\n",
       "      <td>2, https://miro.medium.com/v2/resize:fit:640/f...</td>\n",
       "      <td>4</td>\n",
       "      <td>231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>547</td>\n",
       "      <td>43619</td>\n",
       "      <td>https://medium.com/messaricrypto/derivatives-i...</td>\n",
       "      <td>Derivatives in Crypto</td>\n",
       "      <td>Futures and options and swaps, oh my!</td>\n",
       "      <td>Jack PurdyFollowMessari Crypto--ListenShareThi...</td>\n",
       "      <td>Jack Purdy</td>\n",
       "      <td>https://medium.com/@jackpurdy</td>\n",
       "      <td>1, https://miro.medium.com/v2/resize:fit:640/f...</td>\n",
       "      <td>3</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>548</td>\n",
       "      <td>19291</td>\n",
       "      <td>https://medium.com/bloomberg/samsungs-reputati...</td>\n",
       "      <td>Samsungâ€™s Reputation Founders on Rush for Lead...</td>\n",
       "      <td>None</td>\n",
       "      <td>Member-only storyBloombergFollowBloomberg--4Sh...</td>\n",
       "      <td>Bloomberg</td>\n",
       "      <td>https://medium.com/@bloomberg</td>\n",
       "      <td>1, https://miro.medium.com/v2/resize:fit:640/f...</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>434 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  Index                                                URL  \\\n",
       "0             0  12925  https://medium.com/@dalwrobinson/where-the-str...   \n",
       "1             1  42280  https://medium.com/microsoft-cybersecurity/wei...   \n",
       "2             2   1753  https://medium.com/@rohela99/microservice-canv...   \n",
       "3             4   6769  https://medium.com/@chevallier/the-road-to-cto...   \n",
       "4             5  28240  https://medium.com/@roblevintennis/this-was-fu...   \n",
       "..          ...    ...                                                ...   \n",
       "429         543  31523  https://medium.com/active-theory/netflix-our-p...   \n",
       "430         544  48943  https://medium.com/@livewellmumbai/android-10-...   \n",
       "431         545  12711  https://medium.com/@avonleafisher/go-live-in-a...   \n",
       "432         547  43619  https://medium.com/messaricrypto/derivatives-i...   \n",
       "433         548  19291  https://medium.com/bloomberg/samsungs-reputati...   \n",
       "\n",
       "                                                 Title  \\\n",
       "0                     Where the streets have new names   \n",
       "1                     Weighing Past and Future Success   \n",
       "2                                  Microservice Canvas   \n",
       "3                                 The Road to CTO (#3)   \n",
       "4    This was fun! I challenged myself to do them i...   \n",
       "..                                                 ...   \n",
       "429                                Netflix: Our Planet   \n",
       "430         Android 10 update arrives on the Nokia 8.1   \n",
       "431  â€œGo Live in a Cave if Youâ€™re Really Anti-Capit...   \n",
       "432                              Derivatives in Crypto   \n",
       "433  Samsungâ€™s Reputation Founders on Rush for Lead...   \n",
       "\n",
       "                                             Sub Title  \\\n",
       "0                                                 None   \n",
       "1    Nothing worthwhile is easy. What are you willi...   \n",
       "2                                                 None   \n",
       "3             This ghost wonâ€™t escape me much longer ðŸ‘»   \n",
       "4                                                 None   \n",
       "..                                                 ...   \n",
       "429                                               None   \n",
       "430                                               None   \n",
       "431                                               None   \n",
       "432              Futures and options and swaps, oh my!   \n",
       "433                                               None   \n",
       "\n",
       "                                               Article                 Author  \\\n",
       "0    The Hall PressFollowHall Associated Publicatio...         The Hall Press   \n",
       "1    Lucas DowdFollowMicrosoft Cybersecurity--Liste...             Lucas Dowd   \n",
       "2    RohelaFollow--ListenShareEvery business analys...                 Rohela   \n",
       "3    JÃ©rÃ©my ChevallierFollow--ListenShareItâ€™s encou...      JÃ©rÃ©my Chevallier   \n",
       "4    Ady NgomRob LevinFollow--2ListenShareThis was ...              Rob Levin   \n",
       "..                                                 ...                    ...   \n",
       "429  Member-only storyActive TheoryFollowActive The...          Active Theory   \n",
       "430  Sanket Ramesh PrasadeFollow--ListenShareAndroi...  Sanket Ramesh Prasade   \n",
       "431  Avonlea FisherFollow--ListenShareWhether or no...         Avonlea Fisher   \n",
       "432  Jack PurdyFollowMessari Crypto--ListenShareThi...             Jack Purdy   \n",
       "433  Member-only storyBloombergFollowBloomberg--4Sh...              Bloomberg   \n",
       "\n",
       "                             Author URL  \\\n",
       "0      https://medium.com/@dalwrobinson   \n",
       "1        https://medium.com/@lucas.dowd   \n",
       "2                   https://medium.com/   \n",
       "3        https://medium.com/@chevallier   \n",
       "4                   https://medium.com/   \n",
       "..                                  ...   \n",
       "429    https://medium.com/@activetheory   \n",
       "430  https://medium.com/@livewellmumbai   \n",
       "431                 https://medium.com/   \n",
       "432       https://medium.com/@jackpurdy   \n",
       "433       https://medium.com/@bloomberg   \n",
       "\n",
       "                                             Image URL  Reading Time  \\\n",
       "0                                                    2             8   \n",
       "1    1, https://miro.medium.com/v2/resize:fit:640/f...             2   \n",
       "2    3, https://miro.medium.com/v2/resize:fit:640/f...             6   \n",
       "3    2, https://miro.medium.com/v2/resize:fit:640/0...             2   \n",
       "4                                                    0             1   \n",
       "..                                                 ...           ...   \n",
       "429                                                  0             7   \n",
       "430                                                  1             2   \n",
       "431  2, https://miro.medium.com/v2/resize:fit:640/f...             4   \n",
       "432  1, https://miro.medium.com/v2/resize:fit:640/f...             3   \n",
       "433  1, https://miro.medium.com/v2/resize:fit:640/f...             5   \n",
       "\n",
       "     Clap Count  \n",
       "0            68  \n",
       "1             1  \n",
       "2             0  \n",
       "3            10  \n",
       "4            11  \n",
       "..          ...  \n",
       "429         948  \n",
       "430           0  \n",
       "431         231  \n",
       "432         149  \n",
       "433          17  \n",
       "\n",
       "[434 rows x 11 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraped_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6274dae",
   "metadata": {},
   "source": [
    "### Some of the data we scraped have empty values, some URL links were broken, and some articles were paid, could only be accessed by having a membership account on medium.com."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0381281",
   "metadata": {},
   "source": [
    "# Part 2 - Creating an API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c928a0ec",
   "metadata": {},
   "source": [
    "### API in flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed43008b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'flask'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mflask\u001b[39;00m \u001b[39mimport\u001b[39;00m Flask,request, jsonify\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m app\u001b[39m=\u001b[39m Flask(\u001b[39m__name__\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'flask'"
     ]
    }
   ],
   "source": [
    "from flask import Flask,request, jsonify\n",
    "import pandas as pd\n",
    "\n",
    "app= Flask(__name__)\n",
    "\n",
    "df = pd.read_csv('sample.csv')\n",
    "\n",
    "@app.route('/',methods=['GET'])\n",
    "def index():\n",
    "    return 'Hello World'\n",
    "\n",
    "@app.route('/search', methods=['GET'])\n",
    "def search_titles():\n",
    "    query = request.args.get('query')  \n",
    "    \n",
    "    if not query:\n",
    "        return jsonify({'error': 'No query provided'})\n",
    "\n",
    "    results = df[df['Title'].str.contains(query, case=False)]\n",
    "    \n",
    "    results_list = results.to_dict(orient='records')\n",
    "    \n",
    "    return jsonify({'results': results_list})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825a2c86",
   "metadata": {},
   "source": [
    "### Thank You!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
